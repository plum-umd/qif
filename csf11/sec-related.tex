
\section{Discussion and related work}
\label{sec:related}

Prior work aimed at controlling access to users' private data has
focused on access control policies.  For example,
Persona~\cite{persona} users can store personal data on distributed
storage servers that use attribute-based encryption%
\ifacita
.
\else
; only those
parties that have the attribute keys for particular data items may see
them.
\fi
% XBook~\cite{xbook} enforces users' policies governing
% third-party access to personal data. However, most past approaches suggest a binary access control model.
Our approach relaxes the access control model to offer more
fine-grained information release policies by directly modeling an
attacker's belief.

Others have considered how an adversary's knowledge of private data
might be informed by a program's output.  Clark, Hunt, and
Malacaria~\cite{clark2005QIF} define a static analysis that bounds the
secret information a straight-line program can leak in terms of
equivalence relations between the inputs and outputs.  Backes et
al.~\cite{backes09automatic} automate the synthesis of such
equivalence relations and quantify leakage by computing the exact size
of equivalence classes.  K\"opf and
Rybalchenko~\cite{kopf:rybalchenko} extend this approach, improving
its scalability by using sampling to identify equivalence classes and
using under- and over-approximation to obtain bounds on their size.
% \sbmcomment{Here we imply that the size of these blocks are important,
% but then Boris said that it is only the number of blocks that is of interest.
% Could we clarify this?} \mwh{They worry about class sizes to compute
% shannon entropy.  It's mentioned here because we care about sizes
% too.  I think we can just leave it given clarifications I make below.}
Mu and Clark~\cite{Mu:2009:inverval-qif} present a similar analysis
that uses over-approximation only.  In all cases, the inferred
equivalence classes can be used to compute entropy-based metrics of
information leakage.

\ifacita
We differ from this work in two main ways.  First, we implement a
different security criterion.  The most closely related metric is
\emph{conditional vulnerability} $V$ as proposed by Smith~\cite{smith09foundations},
which gives the expected information gain following a query execution.
This is the information gain for each output weighted by the probability of that output.
Our notion of threshold security
(Definition~\ref{def:threshold}) is stronger because it considers each
output individually: if \emph{any} output, however unlikely, would
increase knowledge beyond the threshold, the query would be rejected.

The second difference of our approach as compared to
the prior work in
\cite{clark2005QIF,backes09automatic,kopf:rybalchenko,Mu:2009:inverval-qif}
is that we keep an
on-line model of knowledge according to actual, observed query results,
which increases our precision.
\else
We differ from this work in two main ways.  First, we implement a
different security criterion.  The most closely related metric is
\emph{vulnerability} $V$ as proposed by Smith~\cite{smith09foundations},
which can be defined using our notation as follows:\footnote{Smith
  actually proposes \emph{min entropy}, which is $-\mathit{log}\;V$.}
\begin{definition}
\label{def:vul-threshold}
Let $\delta' = \eval{S}{\delta}$, where $\delta$ is the model of the
querier's initial belief, and let $\overline{\delta_X} \defeq
\normal{\project{\delta}{X}}$.  Then query $S$ is \emph{vulnerability 
  threshold secure} iff for $$V = \sum_{\sigma_L \in
  \nzset{\delta'_L}}\; \overline{\delta'_L}(\sigma_L) \cdot \max_{\sigma_H \in \states_H}\;
  \overline{(\dcond{\delta'}{\sigma_L})_H}(\sigma_H)$$
  we have $V \leq t$ for some threshold $t$.
\end{definition}
% $Pr[H=h] = (\project{\delta}{H})(h)$
% $Pr[H=h|L=l] = (\project{\dcond{\pevalp{S}{\delta}}{(L=l)}}{H})(h)$
% $Pr[L=l] = \dcond{\pevalp{S}{\delta}}{(L=l)}$
The above definition is an \emph{expectation}
over all possible outputs $\sigma_L$, so unlikely outputs have less
influence.  Our notion of threshold security
(Definition~\ref{def:threshold}) is stronger because it considers each
output individually: if \emph{any} output, however unlikely, would
increase knowledge beyond the threshold, the query would be rejected.
For example, recall the query from Example~\ref{ex:bday} where the
secret data $\var{bday}$ is (assumed by the querier to be) uniformly
distributed; call this query $Q_1$.  According to
Definition~\ref{def:vul-threshold}, the minimum acceptable threshold
$t \geq V = 2/365 \approx 0.005$, whereas according to
Definition~\ref{def:threshold}, the minimum threshold is $t \geq 1/7 \approx
0.143$ which corresponds the equivalence class $260 \leq \var{bday} <
267$.

The other main difference is that we keep an
on-line model of knowledge according to prior, actual query results,
which increases our precision.  To see the benefit consider performing
query $Q_1$ followed by a query $Q_2$ which uses the code from
Example~\ref{ex:bday} but has $\var{today} = 265$.  With our system
and $\var{bday} = 270$ the 
answer to $Q_1$ is $\sfalse$ and with the revised belief the query $Q_2$
will be accepted as below threshold $t_d = 0.2$.  If instead we had to
model this pair of queries statically they would be rejected because
(under the assumption of uniformity) the pair of outputs
$\strue$,$\strue$ is possible and implies $\var{bday} \in \{ 265, 266
\}$ which would require $t_d \geq 0.5$.  Our approach also inherits
from the belief-based approach the ability to model a querier who is
misinformed or incorrect, which can arise following the result of a
probabilistic query (more on this below) or because of a change to the
secret data between queries~\cite{clarkson09quantifying}.
% Probabilistic queries are not
% handled by the previously-described existing work that statically
% computes conditional min-entropy.  
On the other hand, these advantages
of our approach come at the cost of maintaining on-line belief models.
\fi

\ifacita
Our abstract domains $\ppolys$ and $ \ppowers $ are useful
beyond the application of belief-based threshold security and prior
work has looking at probabilistic abstract domains for static analysis
of probabilistic programs~\cite{Monniaux_these}.  However, that work
contains no treatment of
distribution normalization, which is crucial for
belief-based information flow analysis.  Sound handling of normalization
requires the use of under-approximations, which is unique to our approach.
\else
Our proposed abstract domains $\ppolys$ and $ \ppowers $ are useful
beyond the application of belief-based threshold security; e.g., they
could be used to model uncertainty off-line (as in the above work)
rather than beliefs on-line, with the advantage that they are not
limited to uniform distributions (as required
by~\cite{backes09automatic,kopf:rybalchenko}).  Prior work on
probabilistic abstract interpretation is insufficient for this
purpose.  For example, Monniaux~\cite{Monniaux_these} gives an
abstract interpretation for probabilistic programs based on
over-approximating probabilities.  That work contains no treatment of
distribution conditioning and normalization, which are crucial for
belief-based information flow analysis.  The use of
under-approximations, needed to soundly handle 
normalization, is unique to our approach.
\fi

McCamant and Ernst's \flowcheck{} tool~\cite{McCamantE2008} measures
the information released by a particular execution.  However, it
measures information release in terms of \emph{channel capacity},
rather than remaining uncertainty which is more appropriate for our setting.
\ifacita
\else
For example, \flowcheck{} would report a query that tries to guess a
user's birthday leaks one bit regardless of whether the guess was
successful, whereas the belief-based model (and the other models
mentioned above) would consider a failing guess to convey very little
information (much less than a bit), and a successful guess conveying
quite a lot (much more than a bit).
\fi
% \flowcheck{} also does not distinguish leaks of particular
% information, and lacks the other benefits of the belief-based
% approach.

\ifacita
To avoid reasoning directly about an adversary's knowledge, Dwork and
colleagues proposed \emph{differential privacy}~\cite{diffpriv}: a
differentially private query over a database of individuals' records
is a randomized function that produces roughly the same answer whether
a particular individual's data is in the database or not.  This approach
is not ideal for our setting since it relies on a trusted database curator to
perform the randomization properly, whereas we prefer having users control
access to their own data.
Additionally, it is difficult to see how
to effectively adapt differential privacy, which was conceived for
queries over many records, to queries over an individual's record,
as in our setting.
\else
To avoid reasoning directly about an adversary's knowledge, Dwork and
colleagues proposed \emph{differential privacy}~\cite{diffpriv}: a
differentially private query over a database of individuals' records
is a randomized function that produces roughly the same answer whether
a particular individual's data is in the database or not.  Thus, if
the database curator is trustworthy, there is little reason for an
individual to not supply his data.  However, we prefer users to control access
to their data as they like, rather than have to trust a curator.

In any case, it is difficult to see how
to effectively adapt differential privacy, which was conceived for
queries over many records, to queries over an individual's record,
as in our setting.  To see why,
consider the birthday query from Example~\ref{ex:bday}.  Bob's
birthday being/not being in the query range influences the output of
the query only by 1 (assuming yes/no is 1/0). One could add an
appropriate amount of (Laplacian) noise to the query answer to hide
what the true answer was and make the query differentially
private. However, this noise would be so large compared to
the original range $\{0,1\}$ that the query becomes essentially
useless---the user would be receiving a birthday announcement most
days.\footnote{By our calculations, with privacy parameter $\epsilon =
  0.1$ recommended by 
Dwork~\cite{diffpriv}, the probability the query returns the correct
result is approximately $0.5249$.}   By contrast, our approach permits answering
queries exactly if the release of information is below the threshold.  Moreover,
there is no limit on the number of queries as long the information
release remains bounded; differential privacy, in general, must impose
an artificial limit (termed the \emph{privacy budget}) because it does
not reason about the information released.

Nevertheless, differential privacy is appealing, and it would be
fruitful to consider how to apply its best attributes to our setting.
Rastogi and Suciu~\cite{rastogi09relationship} propose a property
called \emph{adversarial privacy} that suggests a way forward.  Like
our approach, adversarial privacy is defined in terms of a change in
attacker knowledge. Roughly: a query's output on any database may
increase an attacker's a priori belief $\delta(\sigma)$ about any
state $\sigma$ by at most $\epsilon$ for all $\delta \in D$ for some
$D \in \powerset{\dists}$.  Rastogi and Suciu show that, for a certain
class $D$, adversarial privacy and differential privacy are
equivalent, and by relaxing the choice of $D$ one can smoothly trade
off utility for privacy.  We can take the
reverse tack: by modeling a (larger) set of beliefs we can favor
privacy over utility.  Our abstractions $\ppolys$ and $\ppowers$
already model sets of distributions, rather than a single
distribution, so it remains interesting future work to exploit this
representation toward increasing privacy. 
\fi

\ifacita
Another important open question for our work is means to handle
collusion.  Following our motivating example in the Introduction, the
user's privacy would be thwarted if he shared only his birth day with
querier $X$ and only his birth year with $Y$ but then $X$ and $Y$
shared their information.  A simple approach to preventing this would
be to model adversary knowledge globally, effectively assuming that
all queriers share their query results; doing so would prevent either
$X$'s or $Y$'s query (whichever was last).  This approach is akin to
having a global privacy budget in differential privacy and, as there,
obviously harms utility.

This approach to collusion is sound only for deterministic queries
and cannot handle probabilistic queries such as Example~\ref{ex:specyear}.
More work is needed to identify a robust means of handling collusion
that can deal with arbitrary queries.
\else
Another important open question for our work is means to handle
collusion.  Following our motivating example in the Introduction, the
user's privacy would be thwarted if he shared only his birth day with
querier $X$ and only his birth year with $Y$ but then $X$ and $Y$
shared their information.  A simple approach to preventing this would
be to model adversary knowledge globally, effectively assuming that
all queriers share their query results; doing so would prevent either
$X$'s or $Y$'s query (whichever was last).  This approach is akin to
having a global privacy budget in differential privacy and, as there,
obviously harms utility.  Dealing with collusion is more problematic
when using probabilistic queries, e.g., Example~\ref{ex:specyear}.
This is because highly improbable results make a querier more
uncertain, so combining querier knowledge can misrepresent individual
queriers' beliefs.  Roughly speaking, querier $X$ could perform a
query $Q$ that misinforms the modeled global belief, but since querier
$Y$'s actual belief is not changed by the result of $Q$ (since he did
not actually see its result), he could submit $Q'$ and learn more than
allowed by the threshold.  Disallowing probabilistic queries
solves this problem but harms expressiveness.  Another option is to more
actively track a set of beliefs, as hinted at above.
\fi

% Following the flow of our example from Section~\ref{sec:overview},
% when advertiser $X$ submits Example~\ref{ex:specyear}, a result of
% $\strue$ will lead $X$ to believe that $1980$ (Bob's actual birthday)
% is a less likely birth year ($1/11814 \times 268 = 0.023$) than he
% previously thought ($1/37 = 0.027$).  Suppose we are using $X$'s
% modeled belief (call it $\delta_X$) to also model $Y$'s knowledge,
% where $Y$ would otherwise give $1980$ has probability $1/37$ (i.e., if
% we were modeling $Y$'s knowledge separately as $\delta_Y$).  If Bob's
% threshold for his birth year is $t_{yr} = 0.03$, then under $\delta_X$
% a query \mwh{need to choose a query in which we'd reject under
%   $\delta_Y$ but not under $\delta_X$}.

% One way to address this problem is to allow only deterministic
% queries.  Then there is no misinformation.  Obviously doing this
% reduces the system's expressive power.  Another possibility is to
% force knowledge to be monotonic: if an accepted query Q would cause
% the posterior certainty to be reduced, we do not change the modeled
% belief, but leave it as is.  I haven't thought carefully about the
% ramifications of doing so, though.  I suspect the result is simply
% conservative, but I need to think harder about whether it's unsafe.

% So, in short, I think that DP has problems with collusion, but these
% problems are entirely in terms of utility, not performance or
% safety.  That is, DP, can have a global privacy budget and while
% using such a mechanism will severely limit utility, it will not harm
% privacy.  In our case, we could model many belief sets to avoid the
% safety problems, but at an enormous performance cost, and at some
% cost to utility (the same flavor as DP, i.e., if principals were not
% really colluding).  Or, we could ignore the possibility of collusion
% to regain performance and utility, but impose some risk on safety.

% But there is a key difference to your approach: [25,26] use *conditional* min-entropy as a measure, which is (in some sense) the expected min-entropy over all possible choices of secrets. An iterative application (and summing conditional min-entropy, as suggested above) implicitly assumes that the secret is freshly chosen in each run. While this is a sound over-approximation of what the adversary may learn, its knowledge will never level off (as it may do in your approach). This is a clear advantage of the belief-based approach.

%%In any case, we believe there is much to be said for both approaches, and hope that this paper serves as another useful step on the path toward practical privacy-preserving computation.


%%For example, if a client
%%wants to determine what proportion of Facebook users are male, this
%%query will be permitted, as the removal of a particular user's data
%%will not significantly affect the result.  However in the course of
%%computing this result, substantial access to individual user data is
%%required.   In the absence of a system for secure multi-party
%%computation \cite{fairplaymp}, the agent computing the result must
%%have full access to gender information for each user, so that the
%%number of male users and female users can be determined.  Such queries
%%would leak too much individual information.

%%Thus, it would appear that we need a trusted third party to execute
%5these queries.  However, in our approach one can use the probabilistic conditional
%%\textsf{pif} to permit certain types of data aggregation.  For our
%%Facebook gender example, an estimate of the proportion of users who
%%are male can be obtained by executing the query below.  We assume that
%%gender is encoded as a binary value, where 0 represents ``male.''

%%\[\spif{0.6}{output := gender}{output := not(gender)}\]

%% This gives the client imperfect information about each user, but this
%5information is still useful in aggregate. For 60\% of the queries, the
%%client will get the correct gender.  If all users are male, the client would
%%expect 60\% of the queries to return 0.  If all users are female, the client
%%would expect 40\% of the queries to return 0.  The percentage in between
%%correspond to varying percentages of male users, with the crossover point
%%being 50\%---if more than 50\% of the queries return 0, then there are likely
%%to be more males than females (where the likelihood depends on the number
%%of users that are sampled).

%%Second, our approach investigates a knowledge-based policy that succinctly captures the semantics of the information that has thus far been revealed to a querier via one or more queries. While differential privacy allows the queried entity to cumulatively track loss in privacy over one or more queries~\cite{PINQ-SIGMOD09}, it fails to consider semantic overlap between such queries $-$ thereby weakening the tightness of their estimates.


%% to pass the individual privacy
%%monitoring \sbmcomment{better term for this?} system described
%%here.



%% Recent advances have explored the notion of adversarial privacy~\cite{rastogi09relationship} to partially bridge this semantic gap by explicitly modeling the adversary's belief on the input dataset. In their setting privacy is defined by the closeness between the posterior and prior distribution on the likelihood of a tuple after seeing the output of query. Their approach is shown to offer tighter estimates by proving that adversarial privacy under a bounded belief model subsumes differential privacy. While their approach quantitatively estimates privacy loss wrt to the querier's belief (as against differential privacy that is agnostic to the querier's belief), they fail to capture the semantics of the information released via one or more queries. Further, similar to differential privacy, adversarial privacy is designed to operate on aggregate data from multiple individuals.

%%Differential privacy~\cite{diffpriv}: works on aggregate data, not on
%%individual data; fixed number of possible queries allowed until the
%%data must be destroyed (cannot even answer the same query twice).  By
%%contrast, our approach models semantic knowledge, so you can ask the
%%same question in different ways and still get an answer.

%%Also want to talk about Rastogi and Suciu's stuff.

%%Backes et al.~\cite{backes09automatic} model information leakage in a program as (the size of) an equivalence relationship between the set of input and output symbols. While their approach quantifies per-program entropy measures, they also fail to offer fine-grained (per query) semantically richer approach to track information release.

%%\mwh{More rambling below}

%%Diffpriv different in that they don't model belief explicitly, but they do have a privacy ``budget'' $\epsilon$.  In effect this is a counter of the amount of knowledge they are willing to let an attacker gain.  While they are not required to model a belief explicitly, which is an advantage, they need to have the assumed level of knowledge in mind in order to set $\epsilon$.  A disadvantage of the indirect modeling of knowledge is that it is difficult to reason how much a query $q$ overlaps with a prior query $q'$.  As such, the default is to presume all queries reveal (equivalent) knowledge, and so there is a fixed number of queries a particular principal may pose.  Also, equally vulnerable to collusion as our approach.  I wonder: how does diffpriv handle changing values?  Work on declassification/anonymization: picks values to release in advance, so less flexibility for the querier.  In scenarios like SMC, this could be a showstopper.  But this is simpler to implement and resistant to collusion.  Could imagine combining pro-active anonymization with belief-based policies.

%%\mwh{Taken from introduction, to work in here}:


%%Returning exact results is in contrast to approaches like differential privacy~\cite{diffpriv}
%%that, while also limiting which queries may be performed, must add
%%noise to increase uncertainty, since knowledge is not modeled
%%directly.

%%\todo{Mention limitations somewhere: what about collusion?
%%  What if you get the belief models wrong?  How to deal with
%%  non-discrete data, or fixed-length loops?}

%%\mwh{Work the following into the discussion of differential privacy?}



%%\mwh{and more ...}

%%Differential privacy guarantees that privacy is preserved by ensuring
%%that sufficient data aggregation occurs.  

%%\sbmcomment{Would be good to make this more formal.  What are the equations
%%that yield the likely percentage of males and the confidence in this estimate?}

