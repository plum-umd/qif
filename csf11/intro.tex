\section{Introduction}

% MWH: Changed the first paragraph to reflect these observations: I
% don't believe third-party apps do the targeted ads, Facebook does.
% Also, I don't think we should indicate that Facebook might be trusted.
% We want to lump it with apps as possibly untrusted.  That's what the
% myspace data comment, and the EULA comment, was about.

Facebook, Twitter, Flickr, and other successful on-line services
enable users to easily foster and maintain
relationships by sharing information with friends and fans.  These
services store users' personal information and use it to customize the
user experience and to generate revenue.  For example, Facebook
third-party applications are granted access to a user's ``basic'' data
(which includes name, profile picture, gender, networks, user ID, and list
of friends~\cite{facebook-apps}) to implement services like
birthday announcements and horoscopes, while Facebook selects
ads based on age, gender, and even sexual preference~\cite{guha10}.
Unfortunately, once personal information is collected, users have
limited control over how it is used.  For example, Facebook's EULA
grants Facebook a non-exclusive license to any content a user
posts~\cite{facebook-tos}.  MySpace, another social network site, has recently
begun to sell its users' data~\cite{myspace-data}.  

Some researchers have proposed that, to
keep tighter control over their data, users could use a storage server
(e.g., running on their home network) that handles personal data requests,
and only responds when a request is deemed safe~\cite{prpl,persona}.
The question is: which requests are safe?  
% The standard answer is to
% defer to some kind of user-determined access control policy.  
While deferring to user-defined access control policies seems an
obvious approach,  
such policies are unnecessarily restrictive when the goal
is to maximize the customized personal experience.
To see why, consider two example applications: a horoscope
or ``happy birthday'' application that operates on birth month and day,
and a music recommendation algorithm that considers birth year (age).
Access control at the granularity of the entire birth date could
preclude both of these applications, while choosing only to release
birth year or birth day precludes access to one application or the other.
But in fact the user may not care much about these
particular bits of information, but rather about what can be deduced from
them.  For example, it has been reported 
that zip code, birth date, and gender are sufficient information to
uniquely identify 63\% of Americans in the 2000 U.S. census
\cite{golle06revisiting}.  So the user may be perfectly happy to
reveal any one of these bits of information in its entirety as long as
a querier gains no better than a $1 / n$ chance to guess the
entire group, for some parameter $n$.

This paper explores the design and implementation for enforcing what we
call \emph{knowledge-based security policies}.  In our model, a user $U$'s agent responds to
queries involving secret data.  For each querying principal $Q$, the agent
maintains a probability distribution over $U$'s secret data,
representing $Q$'s \emph{belief} of the data's likely values.  For
example, to mediate queries from a social networking site $X$,
user $U$'s agent may model $X$'s otherwise
uninformed knowledge of $U$'s birthday according to a likely
demographic: the birth month and day are uniformly distributed, while
the birth year is most likely between 1956 and
1992~\cite{facebook-demographics}.  Each querier $Q$ is also assigned a
knowledge-based policy, expressed as a set of thresholds, each
applying to a different group of (potentially overlapping) data.  For
example, $U$'s policy for $X$ might be a threshold of $1/100$ for
the entire tuple $(\var{birthdate},\var{zipcode},\var{gender})$, and
$1/5$ for just birth date.  $U$'s agent refuses any
queries that it determines could increase $Q$'s ability to guess a
secret above the assigned threshold.  If deemed safe, $U$'s agent returns the
query's (exact) result and updates $Q$'s modeled belief appropriately.
(We touch upon the risk of colluding queriers shortly.)

To implement our model, we need (1) an algorithm to check whether
answering a query could violate a knowledge-based policy, (2) a method
for revising a querier's belief according to the answer that is given,
and (3) means to implement (1) and (2) efficiently.  We build on the
work of Clarkson et al.~\cite{clarkson09quantifying} (reviewed in
Section~\ref{sec:belief}), which works out the theoretical basis for
(2).  The main contributions of this
paper, therefore, in addition to the idea of knowledge-based policies,
are our solutions to problems (1) and (3). 

Given a means to revise querier beliefs based on prior answers,
it seems obvious how to check that a query does not reveal too much: $U$ runs the query, tentatively
revises $Q$'s belief based on the result, and then responds with the
answer only if $Q$'s revised
belief about the secrets does not exceed the prescribed thresholds.
Unfortunately, with this approach the decision to deny depends on
the actual secret, so a rejection could leak information.  We give an
example in the next section that shows how the entire secret could be
revealed.  Therefore, we propose that a
query should be rejected if there exists \emph{any} possible secret
value that could induce an output whereby the revised belief would exceed the
threshold.  This idea is described in detail in Section~\ref{sec:policy}.

To implement belief tracking and revision, our first thought was to use
languages for probabilistic computation and conditioning, which provide the
foundational elements of the approach.  Languages we know
of---IBAL~\cite{pfeffer07ibal}, Probabilistic
Scheme~\cite{radul07probscheme}, and several other
systems~\cite{park08sampling,goodman08church,kiselyov09embedded}---are
implemented using sampling.  Unfortunately, we found these implementations
to be inadequate because they either underestimate the querier's knowledge
when sampling too little, or run too slowly when the state space is large.

Instead of using sampling, we have developed an implementation based on
abstract interpretation.  
\ifacita
In particular, we develop a novel
\else
In Section~\ref{sec:absinterp} we develop a novel
\fi
abstract domain of \emph{probabilistic polyhedra}, which extends the
standard convex polyhedron abstract domain~\cite{CousotHalbwachs78-POPL} with
measures of probability.  We represent beliefs as a set of probabilistic
\ifacita
polyhedra.
\else
(as developed in Section~\ref{sec:probset}).
\fi
While some prior work has explored probabilistic abstract
interpretation~\cite{monniaux00prob}, this work does not support belief
revision, which is required to track how observation of outputs affects a
querier's belief.  Support for revision requires that we maintain both
under- and over-approximations of the querier's belief,
whereas~\cite{monniaux00prob} deals only with over-approximation.  We have
developed an implementation of our approach based on Parma~\cite{parma} and
LattE~\cite{latte}, which we present in Section~\ref{sec:impl} along with
some experimental measurements of its performance.  We find that while the
performance of Probabilistic Scheme degrades significantly as the input
space grows, our implementation scales much better, and can be orders of
magnitude faster.

Knowledge-based policies aim to ensure that an attacker's knowledge of a secret does not
increase much when learning the result of a query.  Much prior work
aims to enforce similar properties by tracking information
leakage quantitatively~\cite{McCamantE2008, smith09foundations,
  backes09automatic, kopf:rybalchenko, rastogi09relationship}. Our
approach is more 
precise (but also more resource-intensive) because it maintains an on-line model
of adversary knowledge.  An alternative to knowledge-based
privacy is \emph{differential privacy}~\cite{diffpriv} (DP), which requires
that a query over a database of individuals' records produces roughly
the same answer whether a particular individual's data is in the
database or not---the possible knowledge of the querier, and the
impact of the query's result on it, need not be directly considered.
As such, DP avoids the danger of mismodeling a querier's knowledge
and as a result inappropriately releasing information.  DP also ensures
a high degree of compositionality, which provides some assurance
against collusion.  However, DP applies once an individual has
released his personal data to a trusted third party's database, a
release we are motivated to avoid. Moreover, applying DP to queries
over an individual's data, rather than a population, introduces so much
noise that the results are often useless.  We discuss these
issues along with other related work in Section~\ref{sec:related}.

The next section presents a technical overview of the rest of the
paper, whose main results are contained in
Sections~\ref{sec:belief}--\ref{sec:impl}, with further discussion and
ideas for future work in Sections~\ref{sec:related}
and~\ref{sec:conc}.  \ifacita Due to space limitations, we elide all
technical discussion of probabilistic polyhedra; a full development
and a more extensive empirical evaluation are given in our technical
report~\cite{TR}.\footnote{Another version of
this paper was presented at the Computer Security Foundations
Symposium~\cite{mardziel11belief}.}   \fi

% In summary, this paper makes two contributions:
% \begin{itemize}
% \item We propose the use of knowledge-based security policies for
%   protecting private information.  We show how to implement such a
%   policy safely using what amounts to min-entropy.

% \item We propose a means of implementing knowledge-based policies
%   based on abstract interpretation, using a novel construct called a
%   probabilistic polyhedron.  We prove our approach is sound and we
%   show its cost improves that of state-of-the-art
%   approaches. \mwh{make sure the previous is right!}
% \end{itemize}

